---
layout: default
title: "Judging and Rules"
permalink: /rules/
---

## Judging Rubrics

### Pre-Existing Use Case

| KPI | Weight | How judges / infra assign the raw score | Point bands & descriptors |
|-----|-------:|-----------------------------------------|---------------------------|
| **Best Δ vs Baseline** | 40 | • Evaluation service re-runs the team’s single best model on the benchmark.<br>• Improvement % = (team score – baseline) ÷ baseline × 100.<br>• We bucket that % into a tier → points below. | ≥ +15 % → 38-40 pts (Excellent)<br>+10-14.9 % → 32-37 pts (Strong)<br>+5-9.9 % → 24-31 pts (Solid)<br>0-4.9 % → 13-23 pts (Needs lift)<br>&lt; 0 % → 0-12 pts (Below) |
| **Blend Ingenuity** <br>(novel pairing, clever slice selection) | 30 | Judges rate originality & strategic value of the blend (0-20).<br>Sub-criteria:<br>a) Novel dataset pairing<br>b) Smart weighting/slice logic<br>c) Clear rationale for each choice. | 27-30 pts (Highly inventive)<br>21-26 pts (Innovative)<br>15-20 pts (Competent)<br>7-14 pts (Routine)<br>0-6 pts (Unoriginal) |
| **Presentation & Impact Story** | 15 | Panel of judges scores live pitch on five sub-criteria (0-2 pts each):<br>a. Problem Fit & Use-Case<br>b. Clarity of Blend Logic<br>c. Demo Depth (live or recorded)<br>d. Feasibility / Next Steps<br>e. Visual & Narrative Quality<br>• Total (0-10) averaged across judges. | 14-15 pts (Compelling)<br>11-13 pts (Solid)<br>8-10 pts (Basic)<br>3-7 pts (Unclear)<br>0-2 pts (Not shown) |
| **Documentation & Reproducibility** | 15 | Judges verify recipe completeness & ability to reproduce results from provided logs/scripts. | 13-15 pts (Fully reproducible)<br>10-12 pts (Minor gaps)<br>7-9 pts (Partial)<br>3-6 pts (Significant gaps)<br>0-2 pts (Not reproducible) |


## New Use Case

| KPI | Weight | How judges assign the raw score | Point bands & descriptors* |
|-----|-------:|---------------------------------|----------------------------|
| **Use-Case Value & Impact** | 30 | Panel scores 0-30 on:<br>a) Problem significance (who cares & why?)<br>b) Expected ROI / strategic fit for NVIDIA & DataFactory<br>c) Breadth of downstream adoption paths | 25-30 pts (Transformative)<br>19-24 pts (High impact)<br>13-18 pts (Moderate)<br>6-12 pts (Limited)<br>0-5 pts (Unclear) |
| **Blend Ingenuity** <br>(novel pairing, clever slice logic) | 25 | Judges rate 0-25 on:<br>a) Novel dataset combinations<br>b) Smart weighting / slicing choices<br>c) Clear justification of each component | 21-25 pts (Highly inventive)<br>16-20 pts (Innovative)<br>11-15 pts (Competent)<br>5-10 pts (Routine)<br>0-4 pts (Unoriginal) |
| **Technical Feasibility & Validation Plan** | 20 | Judges score 0-20 on:<br>a) Soundness of the proposed modelling approach<br>b) Credible evaluation strategy (fair, not gameable)**<br>c) Realistic resource/risk assessment | 18-20 pts (Rock-solid)<br>14-17 pts (Strong)<br>10-13 pts (Workable)<br>4-9 pts (Questionable)<br>0-3 pts (Unviable) |
| **Prototype Evidence & Demo Depth** <br>(Presentation & Impact Story) | 15 | 0-15 based on live/recorded demo:<br>a) Functioning proof-of-concept or qualitative results<br>b) Clarity of walk-through & insights gleaned so far | 13-15 pts (Compelling)<br>9-12 pts (Solid)<br>5-8 pts (Basic)<br>2-4 pts (Limited)<br>0-1 pts (Not shown) |
| **Documentation & Reproducibility** | 10 | Judges verify recipe completeness & ability to reproduce results from provided logs/scripts. | 9-10 pts (Fully reproducible)<br>7-8 pts (Minor gaps)<br>5-6 pts (Partial)<br>2-4 pts (Significant gaps)<br>0-1 pts (Not reproducible) |


## Eligibility rules
- Only organizer provided datasets should be used
- Teams should be between 3-4 people
- [Code of Conduct summary + link to full doc]  
- [Disqualification scenarios]  
